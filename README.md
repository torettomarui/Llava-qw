# Llava-qw
A 8b multimodal large model based on Llava and qwen2.5-instruct. 

You can access this model by huggingface (https://huggingface.co/spaces/torettomarui/Llava-qw).

This multimodal is trained on the datasets same as Llava, but use different vision model and language model. This multimodel use InternViT as vision model and QW-2.5-instruct-7b as language model.

## Performance
This multimodel achieved a score of 0.446 on the MMMU benchmark, surpassing the 0.35 score of Llava-13b.

Llava-qw-8b
![IMG_8371](https://github.com/user-attachments/assets/ac3fc893-3fae-4514-bea1-c34a82a0c078)

Llava-13b
![IMG_8372](https://github.com/user-attachments/assets/43caf9fd-21e2-43c7-9591-e7d23bb4d95d)

llava-qw-38b
![IMG_8497](https://github.com/user-attachments/assets/7a33b99f-6274-4bf9-861a-929a74f3547c)


